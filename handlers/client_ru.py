import os
import openai
openai.api_key = os.getenv('OPENAI_API_KEY')

from create_bot import dp, bot
from aiogram.dispatcher.filters.state import State, StatesGroup
from aiogram import Dispatcher
from aiogram.types import ReplyKeyboardRemove, Message, CallbackQuery
from aiogram.dispatcher.filters import Text
from aiogram.dispatcher import FSMContext


from keyboards.keyboards_eng import menu_kb, in_text_davinci_003_kb, model_settings_kb, max_length_kb, temperature_replykeyboard, temperature_inlinekeyboard, language_kb, model_kb, return_kb, in_gpt_turbo_kb

from keyboards import keyboards_eng, keyboards_ukr, keyboards_ru

from data_base.sqlite_db import Database, All_states


db = Database('user_settings.db')

                    ### MENU HANDLER ###
from image_generation.image_generation_eng import image_generation_eng
from payments.payments_eng import check_balance, process_in_check_balance     


          ### CREATE COMPLETION, SUM COMPLETION ###
def create_completion(chat_history, message):
  completion = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
      {"role": "system", "content": f"You are a helpful assistant called Sparky. Here is the history of your chat with the user '{chat_history}'. Use the chat history only if it's related to the new prompt from a user"},
      {"role": "user", "content": f"{message}"}])
  return completion

def sum_completion(chat_history, new_prompt, new_response):
  sum_completion = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
      {"role": "system", "content": f"You are an assistant who summurizes the most important information from the chat. The summurization should not exceed 500 tokens. The contents to summurize include: chat history, new prompt, new response. Here they are consecutively: 1 - '{chat_history}' 2 - '{new_prompt}'' 3 - '{new_response}'"}])
  return sum_completion

# sum_completion = sum_completion(chat_history, message.text, completion["choices"][0]["message"]["content"])
# completion = create_completion(chat_history, message.text)


                  ### CHAT WITH SPARKY ###
#@dp.message_handler(Text(equals='Chat with Sparkie'))
async def chat_with_sparkie_ru(message: Message):
  user_id = message.from_user.id
  model = db.get_param(user_id, ("model",))
  if model == 'gpt-3.5-turbo':
    await All_states.in_gpt_turbo_ru.set()
    reply_markup = in_gpt_turbo_kb
  elif model == 'text-davinci-003':
    await All_states.in_text_davinci_003_ru.set()
    reply_markup = in_text_davinci_003_kb
  await bot.send_message(user_id, "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤–∞—Å –≤ —á–∞—Ç–µ. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ. –ë—É–¥—å—Ç–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –Ω–∞ –≤–∞—à–µ–º —Å—á–µ—Ç—É.", reply_markup=reply_markup)


          ### CHAT WITH SPARKY in_gpt_turbo handler ###
#@dp.message_handler(state=Form.in_gpt_turbo)
async def in_gpt_turbo_ru(message: Message, state: FSMContext):
  user_id = message.from_user.id
  user_message = message.text
  if user_message == '‚¨ÖÔ∏è':
    await state.finish()
    await bot.send_message(user_id, "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω—É –∏–∑ –æ–ø—Ü–∏–π:", reply_markup=keyboards_ru.menu_kb)
  elif user_message == "üïπ\n–í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å":
    await All_states.in_choose_model_ru.set()
    model = db.get_param(user_id, ("model",))
    await bot.send_message(user_id, f"–í–∞—à–∞ —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å - —ç—Ç–æ {model}.", reply_markup=return_kb)
    await bot.send_message(user_id, "–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω—É –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:", reply_markup=model_kb)
  else:
    try:
      chat_history = db.get_param(user_id, ("chat_history",))
      completion = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
      {"role": "system", "content": f"You are a helpful assistant called Sparky. Here is the history of your chat with the user '{chat_history}'. Use the chat history only if it's related to the new prompt from a user"},
      {"role": "user", "content": f"{message}"}])
      new_response = completion["choices"][0]["message"]["content"]
      sum_completion = openai.ChatCompletion.create(
      model="gpt-3.5-turbo",
      messages=[
      {"role": "system", "content": f"You are an assistant who summurizes the most important information from the chat. The summurization should not exceed 300 tokens, and you should keep it as concise as possible. The contents to summurize include: chat history, new prompt, new response. Here they are consecutively: 1 - '{chat_history}' 2 - '{user_message}'' 3 - '{new_response}'"}])
      sum = sum_completion["choices"][0]["message"]["content"]
      print(sum)
      await bot.send_message(user_id, new_response)
      db.update_param(user_id, ("chat_history", sum))
      tokens = db.get_param(user_id, ("tokens",))
      num_tokens_used = completion["usage"]["total_tokens"] + sum_completion["usage"]["total_tokens"]
      print('\nnum_tokens_used:', num_tokens_used)
      db.subtract_tokens(user_id, num_tokens_used)
      await bot.send_message(user_id, f"–ë–∞–ª–∞–Ω—Å —Ç–æ–∫–µ–Ω–æ–≤: {tokens-num_tokens_used}")
      print("IN GPT-TURBO MA MAN")
    except Exception as e:
      await bot.send_message(user_id, "–í–æ –≤—Ä–µ–º—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –º–æ–¥–µ–ª—å—é –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –ø–æ–∑–∂–µ.")
      print(e)



            ### CHAT WITH SPARKY in_text_davinci_003 ###
#@dp.message_handler(state=Form.in_text_davinci_003)
async def in_text_davinci_003_ru(message: Message, state: FSMContext):
  user_id = message.from_user.id
  if message.text == '‚¨ÖÔ∏è':
    await state.finish()
    await bot.send_message(user_id, "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω—É –∏–∑ –æ–ø—Ü–∏–π:", reply_markup=menu_kb)
  elif message.text == "‚öôÔ∏è\n–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏":
    await All_states.in_model_settings_ru.set()
    await bot.send_message(user_id, "–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω—É –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫:", reply_markup=model_settings_kb)
  elif message.text == "üïπ\n–í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å":
    await All_states.in_choose_model_ru.set()
    model = db.get_param(user_id, ("model",))
    await bot.send_message(user_id, f"–í–∞—à–∞ —Ç–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å - —ç—Ç–æ {model}.", reply_markup=return_kb)
    await bot.send_message(user_id, "–ú–æ–¥–µ–ª–∏ GPT-3.5 –º–æ–≥—É—Ç –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—É—é —Ä–µ—á—å –∏–ª–∏ –∫–æ–¥.\n\nü§ñ  gpt-3.5-turbo - –°–∞–º–∞—è –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å GPT-3.5, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —á–∞—Ç–æ–≤ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–∞ 1/10 –æ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ text-davinci-003.\n\nü§ñ  text-davinci-003 - –ú–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ª—é–±—É—é –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É –∏ –¥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –∏ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É.", reply_markup=model_kb)
  else:
    try:
    # check if a user's token balance is less than the max length
      tokens, max_length, temperature = db.get_param(user_id, ("tokens", "max_length", "temperature"))
      print(tokens, max_length, temperature)
      if tokens < max_length:
        await bot.send_message(user_id, f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Å—á–µ—Ç—É: {tokens}")
        return
      completion = openai.Completion.create(
      engine="text-davinci-003",
      prompt=(f"{message.text}"),
      max_tokens=max_length,
      n = 1,
      stop=None,
      temperature=temperature,
      )
      await bot.send_message(user_id, completion["choices"][0]["text"])
      num_tokens_used = completion["usage"]["total_tokens"]
      print('num_tokens_used:' ,num_tokens_used)
      db.subtract_tokens(user_id, num_tokens_used)
      await bot.send_message(user_id, f"–ë–∞–ª–∞–Ω—Å —Ç–æ–∫–µ–Ω–æ–≤: {tokens-num_tokens_used}")
    except Exception as e:
      await bot.send_message(user_id, "–í–æ –≤—Ä–µ–º—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –º–æ–¥–µ–ª—å—é –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –ø–æ–∑–∂–µ.")
      print(e)

                ### IN PROMPT SETTINGS HANDLER ###
#@dp.message_handler(state=All_states.in_model_settings)
async def in_prompt_settings_ru(message: Message, state: FSMContext):
  user_id = message.from_user.id
  if message.text == '‚¨ÖÔ∏è':
    await All_states.in_text_davinci_003_ru.set()
    await bot.send_message(user_id, "–í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —á–∞—Ç.", reply_markup=in_text_davinci_003_kb)
  elif message.text == '–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞':
    await All_states.in_max_length_ru.set()
    current_max_length = db.get_param(user_id, ("max_length",))
    await bot.send_message(user_id, f"Maximum length is the maximum number of tokens to generate. One token is roughly 4 characters for normal English text.\nYou can use up to 4097 tokens shared between prompt and completion.\n\nY–í–∞—à–∞ —Ç–µ–∫—É—â–∞—è –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç: {current_max_length}.\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –¥–ª–∏–Ω—É.", reply_markup=max_length_kb)
  elif message.text == '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞':
    await All_states.in_temperature_ru.set()
    current_temperature = db.get_param(user_id, ("temperature",))
    await bot.send_message(user_id, f"–í–∞—à–∞ —Ç–µ–∫—É—â–∞—è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç: {current_temperature}.", reply_markup=temperature_replykeyboard)
    await bot.send_message(user_id, "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –≤–∞–º –∑–Ω–∞—á–µ–Ω–∏–µ:", reply_markup=temperature_inlinekeyboard)

                  ### MAX LENGTH HANDLER ###
#@dp.message_handler(state=All_states.in_max_length)
async def in_max_length_ru(message: Message, state: FSMContext):
  user_id = message.from_user.id
  try:
    max_length = int(message.text)
    if max_length > 4097:
      await bot.send_message(user_id, "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 4097 —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –º–µ–∂–¥—É –≤—Ö–æ–¥—è—â–∏–º —Ç–µ–∫—Å—Ç–æ–º –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º.")
    elif  max_length < 1:
      await bot.send_message(user_id, "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω—å—à–µ 1.")
    else:
      db.update_param(user_id, ("max_length", max_length))
      await All_states.in_model_settings_ru.set()
      await bot.send_message(user_id, f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –±—ã–ª–∞ –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ {max_length}.", reply_markup=model_settings_kb)
  except ValueError:
    if message.text == '‚¨ÖÔ∏è':
      await All_states.in_model_settings_ru.set()
      await bot.send_message(user_id, "–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω—É –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫", reply_markup=model_settings_kb)
    elif message.text == "ÔøΩ":
      current_max_length = db.get_param(user_id, ("max_length",))
      await bot.send_message(user_id, f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ - —ç—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑. –û–¥–∏–Ω —Ç–æ–∫–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞–º –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n–í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ 4097 —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –º–µ–∂–¥—É –≤—Ö–æ–¥—è—â–∏–º —Ç–µ–∫—Å—Ç–æ–º –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º.\n\n–í–∞—à–∞ —Ç–µ–∫—É—â–∞—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç: {current_max_length}\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –≤–∞–º –∑–Ω–∞—á–µ–Ω–∏–µ:", reply_markup=max_length_kb)
    else:
      await bot.send_message(user_id, "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.")




              ### IN TEMPERATURE INLINE HANDLER ###
#@dp.callback_query_handler(lambda c: c.data in ["0", "0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0"], state = All_states.in_temperature)
async def in_temperature_inline_ru(callback_query: CallbackQuery, state: FSMContext):
  user_id = callback_query.from_user.id
  temperature = float(callback_query.data)
  if 0 <= temperature <= 1:
    db.update_param(user_id, ("temperature", temperature))
    await All_states.in_model_settings_ru.set()
    await bot.send_message(user_id, f"–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –±—ã–ª–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ {temperature}.", reply_markup=model_settings_kb)


              ### IN TEMPERATURE REPLY HANDLER ###
#@dp.message_handler(state=All_states.in_temperature)
async def in_temperature_reply_ru(message: Message, state: FSMContext):
  user_id = message.from_user.id
  try:
    temperature = float(message.text)
    if 0 <= temperature <= 1:
      db.update_param(user_id, ("temperature", temperature))
      await All_states.in_model_settings_ru.set()
      await bot.send_message(user_id, f"–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –±—ã–ª–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ {temperature}.", reply_markup=model_settings_kb)
    else:
      raise ValueError
  except ValueError:
    if message.text == "‚¨ÖÔ∏è":
      await All_states.in_model_settings_ru.set()
      await bot.send_message(user_id, "–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–Ω—É –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫:", reply_markup=model_settings_kb)
    elif message.text == "ÔøΩ":
      current_temperature = db.get_param(user_id, ("temperature",))
      await bot.send_message(user_id, f"–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª—å—é. –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–º –∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º, —Ç–æ–≥–¥–∞ –∫–∞–∫ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–æ–∑–¥–∞—Å—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –æ—Ç–≤–µ—Ç—ã. –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∂–µ–ª–∞–µ–º–æ–≥–æ —É—Ä–æ–≤–Ω—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏.\n\n–í–∞—à–∞ —Ç–µ–∫—É—â–∞—è –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç: {current_temperature}. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω–æ–µ –≤–∞–º –∑–Ω–∞—á–µ–Ω–∏–µ:", reply_markup=temperature_inlinekeyboard)
    else:
      await bot.send_message(user_id, "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –æ—Ç 0 –¥–æ 1.", reply_markup=temperature_inlinekeyboard)




            ### IN CHOOSE MODEL INLINE KEYBOARD ###
#@dp.callback_query_handler(lambda c: c.data in ["gpt-3.5-turbo", "text-davinci-003"], state = All_states.in_choose_model)
async def in_choose_model_inline_ru(callback_query: CallbackQuery, state: FSMContext):
  user_id = callback_query.from_user.id
  model = callback_query.data
  reply_markup = None
  if model == "gpt-3.5-turbo":
    db.update_param(user_id, ("model", model))
    await All_states.in_gpt_turbo_ru.set()
    reply_markup=in_gpt_turbo_kb
  elif model == 'text-davinci-003':
    db.update_param(user_id, ("model", model))
    await All_states.in_text_davinci_003_ru.set()
    reply_markup=in_text_davinci_003_kb
  await bot.send_message(user_id, f"–ú–æ–¥–µ–ª—å –±—ã–ª–∞ –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ {model}. –í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —á–∞—Ç.", reply_markup=reply_markup)


          ### IN CHOOSE MODEL REPLY KEYBOARD ###
#@dp.message_handler(state=All_states.in_choose_model)
async def in_choose_model_reply_ru(message: Message, state: FSMContext):
  user_id = message.from_user.id
  current_model = db.get_param(user_id, ("model",))
  if message.text == '‚¨ÖÔ∏è':
    if current_model == "gpt-3.5-turbo":
      chat_keyboard = in_gpt_turbo_kb
      await All_states.in_gpt_turbo_ru.set()
    elif current_model == "text-davinci-003":
      chat_keyboard = in_text_davinci_003_kb
      await All_states.in_text_davinci_003_ru.set()
    await bot.send_message(user_id, "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é –≤–∞—Å –≤ —á–∞—Ç–µ. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ. –ë—É–¥—å—Ç–µ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –Ω–∞ –≤–∞—à–µ–º —Å—á–µ—Ç—É.", reply_markup=chat_keyboard)
  elif message.text == 'ÔøΩ':
    await bot.send_message(user_id, "–ú–æ–¥–µ–ª–∏ GPT-3.5 –º–æ–≥—É—Ç –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—É—é —Ä–µ—á—å –∏–ª–∏ –∫–æ–¥.\n\nü§ñ  gpt-3.5-turbo - –°–∞–º–∞—è –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å GPT-3.5, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è —á–∞—Ç–æ–≤ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–∞ 1/10 –æ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ text-davinci-003.\n\nü§ñ  text-davinci-003 - –ú–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ª—é–±—É—é –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É –∏ –¥–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –∏ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É.", reply_markup=return_kb)



                    #### CHOOSE LANGUAGE ###
#@dp.message_handler(text='üåê')
async def in_choose_lang_reply_ru(message: Message):
  await All_states.in_choose_lang_ru.set()
  user_id = message.chat.id
  user_lang = db.get_param(user_id, ("language",))
  print("choose_lang_reply_ru", user_lang)
  language_messages = {
    'ENG': ("Please choose your preferred language:", keyboards_eng.language_kb),
    'UKR': ("–ë—É–¥—å –ª–∞—Å–∫–∞, –æ–±–µ—Ä—ñ—Ç—å –≤–∞—à—É –º–æ–≤—É:", keyboards_ukr.language_kb),
    'RU': ("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–±–µ—Ä–∏—Ç–µ –≤–∞—à —è–∑—ã–∫:", keyboards_ru.language_kb)
}

  if user_lang in language_messages:
    message_text, reply_markup = language_messages[user_lang]
  else:
    message_text, reply_markup = language_messages['ENG']

  await bot.send_message(user_id, message_text, reply_markup=reply_markup)


              ### PROCESS CHOOSE LANGUAGE ###
#@dp.callback_query_handler(lambda c: c.data in ["ENG", "UKR", "RU"], state = All_states.in_choose_lang_ru)
async def in_choose_lang_inline_ru(callback_query: CallbackQuery, state: FSMContext):
  # add a user's telegram id to the database and set the token balance to 20000, token limit to 100 and temperature TO 0.5
  user_id = callback_query.from_user.id
  lang = callback_query.data
  print("choose_lang_inline_ru", lang)
  current_lang = db.get_param(user_id, ("language",))
  tokens = 20000
  max_length = 100
  temperature = 0.5
  model = 'gpt-3.5-turbo'

  language_messages = {
    'ENG': ("English has been installed.", f"Welcome to Sparky family! We're excited to have you join our community. As a new user, you have been credited with {tokens} tokens to try out our product. We hope you enjoy using our service. Let us know if you have any questions or feedback. Push one of the menu buttons to get started.", keyboards_eng.menu_kb),
    'UKR': ("–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —É–∫—Ä–∞—ó–Ω—Å—å–∫—É –º–æ–≤—É.", f"–õ–∞—Å–∫–∞–≤–æ –ø—Ä–æ—Å–∏–º–æ –¥–æ —Ä–æ–¥–∏–Ω–∏ Sparky! –ú–∏ —Ä–∞–¥—ñ –≤—ñ—Ç–∞—Ç–∏ –≤–∞—Å –≤ –Ω–∞—à—ñ–π —Å–ø—ñ–ª—å–Ω–æ—Ç—ñ. –Ø–∫ –Ω–æ–≤–æ–º—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É, –≤–∞–º –±—É–ª–æ –Ω–∞—Ä–∞—Ö–æ–≤–∞–Ω–æ {tokens} —Ç–æ–∫–µ–Ω—ñ–≤, —â–æ–± –≤–∏–ø—Ä–æ–±—É–≤–∞—Ç–∏ –Ω–∞—à –ø—Ä–æ–¥—É–∫—Ç. –ú–∏ —Å–ø–æ–¥—ñ–≤–∞—î–º–æ—Å—è, —â–æ –≤–∞–º —Å–ø–æ–¥–æ–±–∞—î—Ç—å—Å—è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–∞—à —Å–µ—Ä–≤—ñ—Å. –î–∞–π—Ç–µ –Ω–∞–º –∑–Ω–∞—Ç–∏, —è–∫—â–æ —É –≤–∞—Å —î —è–∫—ñ—Å—å –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –≤—ñ–¥–≥—É–∫–∏. –ù–∞—Ç–∏—Å–Ω—ñ—Ç—å –æ–¥–Ω—É –∑ –∫–Ω–æ–ø–æ–∫ –º–µ–Ω—é, —â–æ–± –ø–æ—á–∞—Ç–∏.", keyboards_ukr.menu_kb),
    'RU': ("–†—É—Å—Å–∫–∏–π —è–∑—ã–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.", f"–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ —Å–µ–º—å—é Sparky! –ú—ã —Ä–∞–¥—ã, —á—Ç–æ –≤—ã –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–∏–ª–∏—Å—å –∫ –Ω–∞—à–µ–º—É —Å–æ–æ–±—â–µ—Å—Ç–≤—É. –í –∫–∞—á–µ—Å—Ç–≤–µ –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–∞–º –±—ã–ª–æ –Ω–∞—á–∏—Å–ª–µ–Ω–æ {tokens} —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—à–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞. –ú—ã –Ω–∞–¥–µ–µ–º—Å—è, —á—Ç–æ –≤–∞–º –ø–æ–Ω—Ä–∞–≤–∏—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à —Å–µ—Ä–≤–∏—Å. –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –æ—Ç–∑—ã–≤—ã, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–æ–±—â–∏—Ç–µ –Ω–∞–º. –ù–∞–∂–º–∏—Ç–µ –æ–¥–Ω—É –∏–∑ –∫–Ω–æ–ø–æ–∫ –º–µ–Ω—é, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.", keyboards_ru.menu_kb)
  }

  message_text, welcome_message, reply_markup = language_messages[lang]
  if current_lang is None:
    db.set_default_user_settings(user_id, tokens, max_length, temperature, model, lang)
    await bot.send_message(user_id, welcome_message, reply_markup=reply_markup)

  else:
    await bot.send_message(user_id, message_text, reply_markup=reply_markup)

    db.update_param(user_id, ("language", lang))
    print("Updated language")
  await state.finish()





                  ### REGISTERING HANDLERS ###
from image_generation.image_generation_eng import create_image, create_image_variation

def register_handlers_client_RU(dp : Dispatcher):
  dp.register_message_handler(chat_with_sparkie_ru, text='üí¨ –ß–∞—Ç —Å–æ Sparky')
  dp.register_message_handler(in_gpt_turbo_ru, state=All_states.in_gpt_turbo_ru)
  dp.register_message_handler(in_text_davinci_003_ru, state=All_states.in_text_davinci_003_ru)
  dp.register_message_handler(in_prompt_settings_ru, state=All_states.in_model_settings_ru)
  dp.register_message_handler(in_max_length_ru, state=All_states.in_max_length_ru)
  dp.register_callback_query_handler(in_temperature_inline_ru, lambda c: c.data in ["0", "0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7", "0.8", "0.9", "1.0"], state=All_states.in_temperature_ru)
  dp.register_message_handler(in_temperature_reply_ru, state=All_states.in_temperature_ru)
  dp.register_callback_query_handler(in_choose_model_inline_ru, lambda c: c.data in ["gpt-3.5-turbo", "text-davinci-003"], state=All_states.in_choose_model_ru)
  dp.register_message_handler(in_choose_model_reply_ru, state=All_states.in_choose_model_ru)
  dp.register_message_handler(in_choose_lang_reply_ru, text='üåê')
  dp.register_callback_query_handler(in_choose_lang_inline_ru, lambda c: c.data in ["ENG", "UKR", "RU"], state = All_states.in_choose_lang_ru)

  # dp.register_message_handler(create_image, state=All_states.in_generate_image)
  # dp.register_message_handler(create_image_variation, state=All_states.in_generate_image_variation)
  # dp.register_message_handler(check_balance, text="üíº\nCheck balance")
  # dp.register_message_handler(process_in_check_balance, state=All_states.in_check_balance)






